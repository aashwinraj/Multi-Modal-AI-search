{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d3db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionProductDataSet(Dataset):\n",
    "    def __init__(self,image_dir,style_dir,transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.style_dir = style_dir\n",
    "        self.transform = transform\n",
    "        self.image_files=[f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
    "        self.image_files = self.image_files[:750]  # In __init__ of your Dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        json_path = os.path.join(self.style_dir, img_name.replace(\".jpg\", \".json\"))\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            return None  # Skip unreadable images\n",
    "\n",
    "        # Read and parse JSON\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        # Example: buidata = metadata[\"data\"]  # if you're accessing under 'data' key\n",
    "\n",
    "        brand = data.get(\"brandName\", \"\")\n",
    "        gender = data.get(\"gender\", \"\")\n",
    "        age_group = data.get(\"ageGroup\", \"\")\n",
    "        article_type = data.get(\"articleType\", {}).get(\"typeName\", \"\")\n",
    "        base_colour = data.get(\"baseColour\", \"\")\n",
    "        season = data.get(\"season\", \"\")\n",
    "        year = data.get(\"year\", \"\")\n",
    "        category = data.get(\"masterCategory\", {}).get(\"typeName\", \"\")\n",
    "        sub_category = data.get(\"subCategory\", {}).get(\"typeName\", \"\")\n",
    "        usage = data.get(\"usage\", \"\")\n",
    "        pattern = data.get(\"articleAttributes\", {}).get(\"Pattern\", \"\")\n",
    "        desc = data.get(\"productDisplayName\", \"\")\n",
    "        price = data.get(\"price\", \"\")\n",
    "        discounted_price = data.get(\"discountedPrice\", \"\")\n",
    "        rating = data.get(\"myntraRating\", \"\")\n",
    "        variant = data.get(\"variantName\", \"\")\n",
    "        article_number = data.get(\"articleNumber\", \"\")\n",
    "        fashion_type = data.get(\"fashionType\", \"\")\n",
    "        display_categories = data.get(\"displayCategories\", \"\")\n",
    "        vat = data.get(\"vat\", \"\")\n",
    "        landing_page = data.get(\"landingPageUrl\", \"\")\n",
    "        # Article attributes (add more if needed)\n",
    "        body_size = data.get(\"articleAttributes\", {}).get(\"Body or Garment Size\", \"\")\n",
    "        # Material extraction (example)\n",
    "        material_desc = data.get(\"productDescriptors\", {}).get(\"description\", {}).get(\"value\", \"\")\n",
    "        material = \"polyester and spandex\" if \"polyester\" in material_desc.lower() else \"\"\n",
    "        # Purpose\n",
    "        purpose = \"sportswear\" if \"sports\" in usage.lower() else \"casualwear\"\n",
    "\n",
    "        \n",
    "\n",
    "# Final rich text\n",
    "        full_text = f\"{brand} {category} {sub_category} {gender.lower()} {pattern.lower()} {base_colour.lower()} {article_type.lower()} made of {material} for {season.lower()} {purpose}. {desc}\".strip()\n",
    "        \n",
    "        if not full_text:\n",
    "            return None  # Skip empty text\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, full_text\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c626e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "dataset = FashionProductDataSet(\n",
    "    r'D:\\archive\\fashion-dataset\\fashion-dataset\\images\\\\',\n",
    "    r'D:\\archive\\fashion-dataset\\fashion-dataset\\styles\\\\',\n",
    "    transform=clip_transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "# After creating your dataset object\n",
    "image_filenames = dataset.image_files\n",
    "texts = [dataset[i][1] for i in range(len(dataset))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3815ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Unnormalize function for CLIP transform\n",
    "def unnormalize(img_tensor):\n",
    "    mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(3,1,1)\n",
    "    std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(3,1,1)\n",
    "    return img_tensor * std + mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c9823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# for images, texts in dataloader:\n",
    "#     images = images.to(device)\n",
    "#     texts = clip.tokenize(texts).to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         image_features = model.encode_image(images)\n",
    "#         text_features = model.encode_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e4e2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "all_image_features = []\n",
    "all_text_features = []\n",
    "for images, texts in dataloader:\n",
    "    images = images.to(device)\n",
    "    text_tokens = clip.tokenize(texts).to(device)\n",
    "    with torch.no_grad():\n",
    "        img_feat = model.encode_image(images)\n",
    "        txt_feat = model.encode_text(text_tokens)\n",
    "        img_feat = F.normalize(img_feat, dim=-1)\n",
    "        txt_feat = F.normalize(txt_feat, dim=-1)\n",
    "    all_image_features.append(img_feat.cpu())\n",
    "    all_text_features.append(txt_feat.cpu())\n",
    "\n",
    "all_image_features = torch.cat(all_image_features, dim=0)\n",
    "all_text_features = torch.cat(all_text_features, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0748321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         ...,\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303],\n",
      "         [1.9303, 1.9303, 1.9303,  ..., 1.9303, 1.9303, 1.9303]],\n",
      "\n",
      "        [[2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         ...,\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749],\n",
      "         [2.0749, 2.0749, 2.0749,  ..., 2.0749, 2.0749, 2.0749]],\n",
      "\n",
      "        [[2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         ...,\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459],\n",
      "         [2.1459, 2.1459, 2.1459,  ..., 2.1459, 2.1459, 2.1459]]]), 'Palm Tree Apparel Bottomwear women printed white skirts made of  for summer casualwear. Palm Tree Girls Sp Jace Sko White Skirts')\n"
     ]
    }
   ],
   "source": [
    "print(dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f22192aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feaetures:\n",
      " torch.Size([750, 512])\n",
      "torch.Size([750, 512])\n",
      "torch.Size([750, 750])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "image_features = F.normalize(all_image_features, dim=-1)\n",
    "text_features = F.normalize(all_text_features, dim=-1)\n",
    "\n",
    "similarity = image_features @ text_features.T  # shape [batch, batch]\n",
    "\n",
    "print(\"Image feaetures:\\n\",image_features.shape)\n",
    "print(text_features.shape)\n",
    "\n",
    "print(similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1bc426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "matches = similarity.argmax(dim=1)\n",
    "correct = torch.arange(len(matches)).to(matches.device)\n",
    "accuracy = (matches == correct).float().mean()\n",
    "print(f\"Matching accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "image_features = F.normalize(image_features, dim=1)\n",
    "index = faiss.IndexFlatIP(image_features.shape[1])  # Cosine similarity\n",
    "index.add(image_features.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eed694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1124  339  461 1098   72] \n",
      " [0.3273982  0.323642   0.31344527 0.31073946 0.30726647]\n"
     ]
    }
   ],
   "source": [
    "query = \"red sleeveless t-shirt\"\n",
    "text_token = clip.tokenize([query]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embed = model.encode_text(text_token)\n",
    "    query_embed = F.normalize(query_embed, dim=-1)\n",
    "\n",
    "# Search\n",
    "D, I = index.search(query_embed.cpu().numpy(), k=5)  # top-5 results\n",
    "print(I[0],'\\n', D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f69e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84179df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
